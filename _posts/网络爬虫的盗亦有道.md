---
title: 网络爬虫的盗亦有道
mathjax: false
date: 2020-03-05 22:46:21
updated: 2020-03-05 22:46:21
tags: [爬虫]
categories: [网络爬虫]
toc: true
---



## 1. 网络爬虫的尺寸

| 序号 | 规模 | 数据量 | 爬取速度 | 第三方库  | 目的                  |
| ---- | ---- | ------ | -------- | --------- | --------------------- |
| 1    | 小   | 小     | 不敏感   | Request库 | 爬取网页 玩转网页     |
| 2    | 中   | 中     | 敏感     | Scrapy库  | 爬取网站 爬取系列网站 |
| 3    | 大   | 大     | 非常关键 | 定制开发  | 爬取全网 构建搜索引擎 |

我们平时见到的网络爬虫，小规模的网页爬虫占到了90%以上的比例。

不要看这样的爬虫很小，但是它针对特定网页或一系列的网页，能发挥很大的作用。

## 2. 网络爬虫引发的问题

1. **骚扰问题**：受限于编写水平和目的，网络爬虫将会为web服务器带来巨大的资源开销。
2. **法律风险**：服务器上的数据有产权归属，网络爬虫获取数据后牟利将带来法律风险。
3. **泄露隐私**：网络爬虫可能具备突破简单访问控制的能力，获得被保护数据从而泄露个人隐私。

<!--more-->

## 3. 网络爬虫的限制

1. 来源审查：判断User-Agent进行限制。

    检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问。

2. 发布公告：Robots协议

    告知所有爬虫网站的爬取策略，要求爬虫遵守。

发布公告仅仅是公告的形式，至于网络爬虫是否遵守，由其自身决定。

通过上面两个方法，互联网实现了对网络爬虫在技术上和道德上的有效限制。

## 4. Robots协议

**Robots Exclusion Standard** 网络爬虫排除标准

作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。

形式：在网站根目录下的robots.txt文件。

案例：京东网站的 https://www.jd.com/robots.txt 内容

```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

### 4.1 Robots协议的基本语法

```shell
# 代表注释，* 代表所有，/ 代表根目录
User-agent: * 
Disallow: /
```

`User-agent`：表示用户。

`Disallow`：表示不允许此用户访问的目录。

其他案例：

http://www.baidu.com/robots.txt

http://news.sina.com.cn/robots.txt

http://www.qq.com/robots.txt

http://news.qq.com/robots.txt

http://www.moe.edu.cn/robots.txt （无robots协议）

注：无robots协议意味着此网站允许任何人对其不限制的进行爬虫。

### 4.2 Robots协议的使用

网络爬虫：自动或人工识别robots.txt，再进行内容爬取。

约束性：Robots协议是建议但非约束性的。网络爬虫可以不遵守，但存在法律风险。

### 4.3 对Robots协议的理解

爬取网页/玩转网页：访问量很小，可以遵守；访问量很大，建议遵守。

爬取网站/爬取系列网站：非商业且偶尔，建议遵守；商业利益，必须遵守。

爬取全网：必须遵守。

