---
title: GFS 论文阅读笔记
date: 2021-12-28 23:31:39
updated: 2022-03-01 01:16:39
categories: [论文阅读笔记]
tags: [GFS,论文,分布式]
mathjax: true
toc: true
---



传送门: [GFS 论文原文](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/035fc972c796d33122033a0614bc94cff1527999.pdf)



## 1. 什么是 GFS



GFS，全称 Google File System，谷歌文件系统。

这篇论文是 2003 年发表的，在这之前，GFS 已经大规模应用在了 Google 内部。

GFS 是 Google 提出的一个文件系统，其是分布式的，主要用于处理越来越庞大的数据。因为当数据量大到一定程度时，传统的数据存储与处理方式就显得很笨重了，不适用了（比如你很难很快地读取数百 TB 的数据）。



<!--more-->





## 2. 设计概述



### 2.1. 假想（目标）

GFS 在设计的时候有一些假想，即预期要实现的目标。

1. 这个系统由很多廉价的、经常会故障的商用组件构建，所以在日常使用中，这个系统必须持续地监控自身，以检测、容忍组件故障，并迅速从组件故障中恢复。
2. 这个系统存储数量适中的大文件。Google 期望是几百万个文件，每个一般是 100MB 或者更大。数 GB 大小的文件在这个系统中也是很常见的，需要高效管理。而小文件肯定也要支持，但是不需要为了这些小文件专门优化。
3. 工作负载主要包括两类读：大文件流的读（流只能顺序读）和小文件的随机读。
    * 大文件流的读：单个读操作一般读几百 KB，更常见的是读 1MB 或者更多。来自同一个客户端连续的读操作经常是从一个文件连续的位置读。
    * 小文件的随机读：一般是在文件的任意位置读几 KB 大小。注重性能的应用程序通常对它们的小读取进行批处理和排序，以逐渐地浏览文件，而不是来回的读（文件指针来回移动）。
4. 这个系统也会有很多大的、连续的写操作，将数据追加到文件末尾。一般这种操作的大小和读差不多。一旦写入操作完成，这个文件很少会再次修改。小的随机写也支持，但是不太高效。
5. 这个系统必须高效地实现定义明确的语义，以支持多客户端并发写入（追加写入）同一个文件。GFS 中的文件通常用作生产者消费者队列或多路合并。系统中有数百个生产者，每个机器上运行一个，这些生产者并发地追加修改一个文件，因此以最小的同步开销来实现原子性是必不可少的。这些文件可能随后被读取，也可能有一个消费者在写的同时读。
6. 高的持续的带宽比低的延迟更重要。GFS 的大多数目标应用程序都重视以高速率批量处理数据，而很少有应用程序对单个读或写有严格的响应时间要求。





### 2.2. 接口

GFS 提供了一个常见的文件系统接口，尽管 GFS 没有实现像 POSIX 这样的标准 API。

GFS 中文件在目录中以层次结构组织，通过路径名区分。

GFS 支持常用操作以创建(create)、删除(delete)、打开(open)、关闭(close)、读(read)和写(write)文件。

此外，GFS 中还有 *snapshot* 和 *record append* 操作。Snapshot 以一个很低的开销创建一个文件的或者一个目录树的拷贝。Record append 允许多个客户端并发地追加写入同一个文件，且确保每个客户端的写入操作都是原子的。Record append 对实现多路合并结果、生产者消费者队列很有用，因为很多客户端可以同时追加写入，而不需要额外的锁。Google 发现在构建大型分布式应用时，这些类型的文件是非常有用的。

Snapshot 和 record append 会在后面进一步讨论。





### 2.3. 架构

一个 GFS 集群包含单个 *master* 和多个 *chunkservers*，允许多个 *client* 访问。如 Figure 1 所示。

![Figure 1: GFS Architecture](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Figure_1.png)

每个 master 或 chunkserver 一般都是一个商品 Linux 机器中运行着的一个用户级服务进程。在同一个机器上同时运行一个 chunkserver 和一个 client 是很容易，但前提是机器资源允许，并且你可以接受运行不稳定的应用程序代码导致的更低的可靠性。

GFS 系统中的文件会被划分为固定大小的 chunks。每个 chunk 使用一个不可变的、全局唯一的 64 位 chunk 句柄来标识，这个 chunk 句柄是在 chunk 创建时由 master 指定的。Chunkservers 在本地磁盘中以 Linux 文件的形式存储 chunks，并读取或写入由 chunk 句柄和字节范围指定的块数据。为了可靠性，每个 chunk 都在多个 chunkservers 上有复制。默认是 3 个复制，但用户可以为文件命名空间的不同部分指定不同的复制级别。

master 维护所有文件系统元数据，包括命名空间、访问控制信息、从文件到 chunk 的映射以及 chunks 当前的位置。master 也会控制系统范围内的活动，比如 chunk 租用管理，孤儿 chunks 的垃圾回收，以及在 chunkservers 之间迁移 chunks。master 会定期在 HeartBeat 消息中与每个 chunkservers 通信，以给 chunkservers 指令并收集其状态信息。

链接到每个应用程序的 GFS 客户端代码中实现了文件系统 API，这个 GFS 客户端代表应用程序与 master 和 chunkservers 通信以读写数据。客户端与 master 交互以进行元数据操作，但所有数据承载通信直接进入 chunkservers。GFS 没有提供 POSIX API，因此不需要连接到 Linux 的 vnode 层。

客户端和 chunkserver 都不缓存文件数据。客户端缓存文件数据几乎没什么好处，因为大多数应用程序通过巨大的文件进行流式传输，或者工作集太大而无法缓存。不缓存文件数据使得客户端代码和总体系统的代码得以简化，因为无需编写代码解决缓存一致性的问题（不过客户端是缓存元数据的）。Chunkservers 不需要缓存文件数据是因为 chunks 是作为本地文件存储的，所以 Linux buffer 缓存已经把频繁访问的数据放在内存中了。



### 2.4. 单个 Master

GFS 中只有一个 master，这大大简化了其设计，并且使得 master 能够根据全局知识做出复杂的 chunk 放置和复制决策。不过必须最小化在读写中 master 的调用次数，防止 master 成为 GFS 系统的性能瓶颈。客户端永远都不会通过 master 读写文件数据，而是向 master 询问该联系哪些 chunkservers，当客户端会在有限的时间内缓存此信息，且直接和 chunkservers 互动，以进行一系列的操作。

---

现在我们通过一个简单的读操作来解释 GFS 的工作流程（就如 Figure 1 中的那样）。

首先，要使用固定的 chunk 大小，客户端把应用程序指定的文件名和字节偏移翻译成这个文件中的一个 chunk 索引。然后客户端向 master 发送一个包含文件名和 chunk 索引的请求，master 给客户端回复相应的 chunk 句柄和 chunk 副本的位置。客户端以文件名和 chunk 索引作为 key 缓存这些信息。

客户端随后给副本之一发送一个请求（大部分情况是最近的一个副本），这个请求中指定了 chunk 句柄和一个 chunk 中的字节范围。同一个 chunk 的读就不再需要 client-master 互动了，直到客户端缓存的信息到期（前文说过在有限的时间内缓存这些信息，也就是说这些信息是有时效性的）或这个文件被重新打开。事实上，客户端往往在一个请求中询问多个 chunks，master 也可以在回复的信息中心包含这些请求的 chunks 信息，这些额外的信息几乎不需要什么额外的开销，就可以避免未来几次的 client-master 交互。





### 2.5. Chunk 大小



Chunk 的大小是关键的设计参数之一。GFS 中将 chunk 的大小设定为 64MB，远远大于一般文件系统的块大小。每个 chunk 副本都以一个普通的 Linux 文件存储在一个 chunkserver 上，只要需要的时候才会扩展。延迟空间分配避免了由于内部碎片造成的空间浪费，这可能是对如此大 chunk 大小的最大反对。

将 chunk 设置为 64MB 这么大，可以提供一个重要的优势。**首先**，减少了客户端与 master 的交互次数，因为在同一个 chunk 上的读和写只需要在最初的请求中向 master 询问一次 chunk 的位置信息。减少客户端与 master 交互次数对于我们的工作负载而言格外重要，因为应用程序往往是连续读写大文件的。即便是对于小的随机读，客户端也可以轻松缓存一个数 TB 工作集的所有 chunk 的位置信息。**第二**，由于一个 chunk 比较大，使得一个客户端更可能在一个给定的 chunk 上执行很多操作，这样就可以在很长的一段时间内，通过保持一个持续的客户端与 chunkserver 之间的 TCP 连接来减少网络开销。**第三**，减少了 master 上存储的元数据大小。这允许我们把元数据放在内存中，把元数据放在内存中又反过来带给我们一些其他的优势，这些优势我们在 2.6.1 中讨论。

另一方面，一个很大的 chunk 大小，即便有延迟空间分配策略，也还是有缺点的。一个小文件可能包含很少数量的 chunks，甚至可能只有一个。这样如果有很多客户端都要访问这同一个文件，那么存储这些 chunks 的 chunkservers 就会成为热点。不过在实践中，热点问题不是主要问题，因为我们的应用程序大多是顺序读多 chunk 的大文件。

然而，当 GFS 首次被批处理队列系统使用时，热点确实出现了：一个可执行文件作为单个 chunk 文件写入 GFS，然后同时在数百台机器上启动。存储此可执行文件的少数 chunkservers 被数百个同时请求过载。Google 通过以更高的复制因子存储此类可执行文件以及使批处理队列系统错开应用程序启动时间来解决此问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。





### 2.6. 元数据



master 中主要存储三种类型的元数据：

1. 文件和 chunk 的命名空间；
2. 从文件到 chunks 的映射；
3. 每个 chunk 的副本的位置。

所有的元数据都存储在 master 的内存里。前两种类型也会通过在操作日志(operation log)上记录修改来持久化，操作日志存储在 master 的本地磁盘上，并且会在远程机器上复制。使用日志使得我们可以容易地、可靠地更新 master 状态信息，而不用承受 master 崩溃导致的不一致性的风险。master 不会持久的存储 chunk 位置信息，而是会在 master 启动时或一个 chunkserver 加入集群时向 chunkserver 询问其 chunks 信息。



#### 2.6.1. 内存中的数据结构

因为元数据存储在内存中，所以 master 的操作是非常快速的。进一步地说，master 定期在后台扫描其整个状态信息是非常简单且高效的。定期扫描是用来实现 chunk 垃圾回收、chunkserver 故障时的重新复制，以及为了负载均衡和跨 chunkserver 使用磁盘空间进行的 chunk 迁移的。4.3 和 4.4 小节会进一步讨论这些内容。

仅在内存访问这些，有一个潜在的问题是，chunks 的数量和整个 GFS 系统的容量受 master 拥有多少内存限制。在实践中这不是一个很严重的限制。对于每个 64MB 的 chunk，master 维护小于 64 字节的元数据。大部分 chunks 是满的，因为大部分文件包含很多个 chunks，只有最后一个可能是不满的。类似地，对于每个文件，master 存储的文件命名空间数据通常少于 64 个字节，因为它使用前缀压缩紧凑地存储文件名。

如果确有必要支持更大的文件系统，只需要给 master 增加额外的内存，这个开销相对于我们在内存中存储元数据获得的简单性、可靠性、性能与灵活性而言，是很小的。



#### 2.6.2. Chunk 的位置



master 不会持有一个持久的关于哪些 chunkservers 有一个给定 chunk 的副本的记录，而是在 master 启动时简单地轮询 chunkservers 来获取这些信息。启动后 master 可以保持最新，因为 master 控制着所有 chunk 的放置，以及通过常规心跳(HearBeat)消息监控着 chunkserver 的状态。

Google 起初尝试在 master 中持久存储 chunk 的位置信息，但是后来决定在 master 启动时（以及启动后定期）从 chunkmasters 请求数据，这简单的多。并且这样做也排除了在有 chunkservsers 加入或离开集群、修改名字，故障、重启时等等保持 master 和 chunkservers 同步的问题。在一个有着数百个服务器的集群上，这些情况常常发生。

另一个理解这样设计决策的思路是这样想，一个 chunkserver 有决定存储哪些 chunks 在其本地磁盘上的最终话语权。在 master 上尝试维护一个这种信息的一致性视图是没有意义的，因为一个 chunkserver 上的错误可能导致 chunk 自发消失（比如磁盘损坏或不可用），或操作员可能修改 chunkserver 的名字。





#### 2.6.3. 操作日志(Operation Log)



操作日志包含至关重要的元数据修改历史记录。

操作日志是 GFS 的核心。操作日志不仅仅是元数据唯一的持久化记录，也是一个逻辑时间线（充当定义并发操作的次序）。文件和 chunks 还有它们的版本(versions，详见 4.5. 小节)，全部由他们被创建时的逻辑时间唯一且永久标识。



由于操作日志是非常重要的，我们必须将其可靠存储，并在在元数据修改持久化之前不让修改对客户端可见。否则，我们会在事实上丢失整个文件系统或最近的客户端操作（即便 chunks 本身还在）（这里原文翻译过来就是这样的。我的理解是客户端对 chunks 的操作依赖其缓存的元数据，如果元数据的改动在持久化前就对客户端可见的话，客户端就会依赖改动后的元数据对 chunks 操作，而此时这些元数据的改动还没有持久化，客户端的操作可能无法执行，导致操作丢失）。因此 GFS 将操作日志在多个远程机器上复制，并且仅在相应的日志记录已经被 flush 到本地和远程磁盘上后才会响应一个客户端的操作。Master 在 flush 前一起批处理几个 log 记录，从而减少 flush 和复制对整个系统吞吐量的影响。

Master 通过重放操作日志来恢复其文件系统状态信息。为了使 master 启动时间最短，就要保持日志小。每当日志超过一个特定的大小时，master 就会生成一个包含其此时状态的检查点(check point)，这样 master 恢复的时候，只需要从本地磁盘加载最近的检查点，然后重放在这个检查点之后的有限数量的日志记录。检查点采用类似 B 树的紧凑形式，可以直接映射到内存中，用于命名空间查找，无需额外解析。 这进一步加快了恢复速度并提高了可用性

因为构建一个检查点需要一些时间，所以 master 的内部状态通过这样的一种方式构造：创建一个新的检查点时不推迟即将到来的修改，master  会切换到一个新的日志文件，并且在一个单独的线程中创建新的检查点。换句话说，新的检查点中包含了切换前的所有修改，切换后的修改会被记录到 master 切换过去的新的日志文件中。对于一个有几百万文件的集群来说，可以在一分钟左右创建完一个新的检查点。当检查点创建完成时，检查点会被写入本地和远程的磁盘。

恢复操作只需要最近的检查点和日志文件序列。更旧的检查点和日志文件就可以随便删了，尽管一般来说会保留一些以抵御灾难。在创建检查点时发生的故障不会影响正确性，因为恢复代码会检查并跳过不完整的检查点。





### 2.7. 一致性模式

GFS 有一个宽松的一致性模型，很好地支持我们的高度分布式应用程序，但是实现起来依然简单且高效。

我们现在讨论 GFS 如何保证一致性，以及这对应用程序来说有何意义。我们也会强调 GFS 如何维护这些保证，但是更详细的内容将在本文的其他部分来说。

#### 2.7.1. GFS 如何保证一致性

文件命名空间的修改（例如，文件创建）是原子的，且只能由 master 来操作：命名空间锁确保原子性和正确性（详见 4.1）；master 的操作日志定义了一个这些操作的全局的总的次序（详见 2.6.3）。

在数据修改后，文件区域的状态依赖于修改的类型，修改成功还是失败，以及这些是否是并发的修改。Table 1 总结了在数据修改后的文件区域的状态。

![Table 1: File Region State After Mutation](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Table_1.png)

* 对于一个文件区域，如果所有的客户端总是看到相同的数据（不论看的是哪个副本），那这个文件区域是一致的 *consistent*。

* 对于一个文件区域，在文件数据修改后，如果这个修改是一致的，并且客户端将看到这个修改写入的全部内容，那么这个文件区域就是 *defined*。

（个人理解：对于一个文件区域，只要所有客户端看到的数据都是一样的，那这个区域就是 consistent 的。在 consistent 的前提下，如果所有修改都已经被写入，就是 defined 的。consistent 是 defined 的子集。即 defined 的一定是 consistent 的，但 consistent 的不一定是 defined 的（上表中的 Recored Append 在后面单独说）。）

当一个修改成功，且没有受到并发写者的干预（即串行的修改），那么受影响的区域是 *defined* 的（且含义一致）：即所有的客户端将总是能看到这个修改写入了什么。

并发的成功的修改使得受影响的区域是 *undefined* 但 *consistent*：即所有的客户端看到的数据是一样的，但这并不意味着每个修改都已经被写入。一般来说，写入的内容由多个修改的混合片段组成。

一个失败的修改会使得文件区域 inconsistent（因此也是 undefined）：不同的客户端在不同的时间可能看到不同的数据。我们在下面描述我们的应用程序如何辨别 defined 的区域和 undefined 的区域。另外，应用程序不需要进一步区分不同种类的 undefined 的区域。

数据修改可能是 *write* 或 *record appends*。

* *write* 使数据被写入在一个由应用程序指定的文件偏移处。
* *record append* 使数据（即 *record*）被**原子地**的追加至少一次（即便是并发修改），但数据写入的文件偏移由 GFS 选择（详见 3.3）。

作为对比，一个普通的 append 仅仅是一个在客户端认为是当前文件末尾的偏移处的 write。

标志着包含写入 record 的 *defined* 的区域的开始的偏移会被返回给客户端。此外，GFS 可能会在写入的内容之间插入填充或 record 的复制。我们认为 GFS 插入内容占据的区域是 *inconsistent* 的（即 Table 1 中的 *defined* interspersed with *inconsistent*，即 *defined* 区域中穿插了 *inconsistent* 区域，但这些区域不会影响读取数据的结果，因为读者会过滤掉这些），且占用的空间比起用户数据的总量而言微不足道。

在连续的成功的修改后，GFS 会保证被修改的文件区域是 *defined* 的，并且包含最后一次修改写入的数据。GFS 实现这一点，通过 (a) 以相同的顺序应用修改到 chunk 以及其所有的拷贝上（详见 3.1），(b) 使用 chunk 版本号检测某个拷贝是否过期（即在其对应的 chunkserver 挂掉时，错过了修改。详见 4.5）。过期的 chunk 拷贝永远都不会被再应用修改，其位置也不会再由 master 提供给客户端，这些过期的 chunk 将尽快被垃圾回收。

由于客户端缓存了 chunk 的位置信息，所以在其缓存的位置信息更新之前，客户端可能会从一个旧的副本中读取数据。只有当缓存条目超时，或文件被重新打开时，这个问题才能解决，因为条目超时或重新打开文件会清除客户端缓存中的所有跟这个文件有关的 chunk 信息。此外，由于我们的文件大多数都是仅 append 的，一个旧的副本通常返回一个最新的 chunk 结束位置之前的位置，而不是过期的数据（也就是说，数据还是有效的数据，只是返回的偏移位置不对）。当一个读者重试并联系 master 时，读者会立即获得现在的 chunk 的位置。

即便在修改成功后的较长时间后，组件故障仍然可以导致数据被损坏、催毁。GFS 通过 master 与所有 chunkservers 定期握手的方式来找到故障的 chunkservers，通过校验和（详见 5.2）来检测数据是否损坏。一旦发现问题，GFS 会尽快通过相应数据的其他有效副本来恢复数据（详见 4.3）。仅当一个 chunk 的所有副本都丢失了，这个 chunk 的丢失才是不可逆地，即便在这种情况下，chunk 也是无法访问，而不是损坏：应用程序会收到一个明确的错误，而不是损坏的数据。





#### 2.7.2. 对应用程序的影响

GFS 应用程序可以用一些其他目的已经需要的简单技术来适应宽松的一致性模型：依赖 append 而不是覆写、检查点，和写入自验证、自识别的记录。

实际上，我们应用程序所有的文件修改，都是通过 append 而不是覆写。

一个典型的用法是，一个写者从头到尾生产一个文件，在数据全部写入完成后，在原子地将文件重命名一个持久化的名字。或者是定期生成检查点，即每成功写入多少数据，就生成一个检查点。检查点也可能包含应用程序级检查和。读者只验证和处理直到最新的检查点的文件区域，即已知的 *defined* 状态的文件区域。无论一致性和并发问题怎样，这种方法都很好地为我们服务。append 比随机写要高效的多，而且在面对应用程序故障时更有弹性。检查点允许写者递增的重新开始（即可以从更新的检查点处接着写），阻止读者处理已经成功写入，但还未对应用程序可见（即在应用程序认为还不完整）的数据。

另一个典型的用法，很多写者并发 append 一个文件，以获取合并结果或作为一个生产者-消费者队列。*Record append* 的 **append 至少一次**语义保留了每个写者的输出。

读者通过下面的方法来处理偶然的填充或者 record 的复制。每个 record 由写者准备好，包含了诸如校验和这种额外信息，这样 record 的有效性就可以验证。读者使用校验和，可以区分填充和 recored 片段。如果应用程序不能容忍偶尔的重复（例如，如果重复的记录会触发非幂等操作），它可以使用记录中的唯一标识符将它们过滤掉，这通常是命名相应应用程序实体（例如 Web 文档）所必需的。这些 record I/O 的功能（除了重复记录的移除）在我们应用程序共享的库代码中，并且也适用其他 Google 的文件接口实现。这样，相同的 record 序列加上极少的重复，总是会被传送给 record 读者。



## 3. 系统交互

Google 设计 GFS 系统交互要最小化在所有的操作中对 master 的涉及（因为 master 只有一个，必须减轻 master 的压力）。在这个背景下，我们现在来说客户端、master 和 chunkserver 如何互动以实现数据修改、原子记录追加(append)，以及快照(snapshot)。



### 3.1. 租约和修改顺序



像 write 或 append 的修改操作是会改变 chunk 的内容或者元数据的。每次修改都会应用在 chunk 的所有副本上。我们使用租约来维护一个在副本之间一致的修改顺序。master 授予一个 chunk 租约给副本之一，我们称这个副本为 *primary*。Primary 为所有修改挑选一个顺序给 chunk。当应用修改时，所有的副本都遵循这个顺序。因此，全局修改顺序先由 master 选择的租约授予顺序定义，在租约内由 priamry 指定的序列号定义。

租约机制是设计用来最小化 master 的管理开销的。一个租约有一个初始的 60s 的超时时间。然而，只要 chunk 被修改，primary 就可以向 master 请求延时，并且通常会收到延时的许可，并且这不限制次数。这些扩展请求与授权是附带在 master 和所有 chunkservers 之间交换的常规的心跳(*HeartBeat*)信息中的。master 有时可能会尝试在一个租约到期前将其撤销（例如，master 想要禁用一个正在重命名的文件上的修改）。即便 master 与一个 primary 失去了联系，master 也可以在旧的租约到期后安全地向另一个副本授予租约。

在 Figure 2 中，我们通过列出 write 控制流描述了这个过程，并且用数字标记了步骤顺序。

![Figure 2: Write Control and Data Flow](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Figure_2.png)

1. 客户端向 master 询问，哪个 chunkserver 持有要访问的 chunk 当前的租约，以及其他副本的位置。如果目前没有任何一个 chunkserver 持有要访问的 chunk 的租约，master 就会选择一个副本，授予一个租约（没有在图上显示出）。
2. master 向客户端回复 primary 的标识和其他副本（图中 *secondary* 标记，所有除了 primary 的副本都是 secondary）的位置。客户端缓存这个数据，用于将来的修改操作。只有当 primary 变得不可达，或副本不再持有租约时，客户端才需要再次联系 master。
3. 客户端把数据 push 给所有的副本，客户端可以以任意的顺序 push。每个 chunkserver 将会在一个内部的 LRU buffer 缓存这些数据，直到这些数据被使用或老化。通过将数据流与控制流解耦，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不管哪个 chunkserver 是 primary 的。我们会在 3.2 进一步讨论这一点。
4. 一旦所有的副本都确认收到了数据，客户端就向 primary 发送一个 write 请求。这个请求标识了早前 push 给所有副本的数据。primary 给其收到的所有修改指定连续的序列号，由于这些修改可能来自多个客户端，所有进行编号是有必要的。primary 按着序号的顺序将修改应用到自己的本地状态。
5. primary 把 write 请求传递给所有的 secondary 副本，每个 secondary 副本以由 primary 指定的同样的序列号顺序应用修改。
6. 向 primary 回复的所有 secondary，表明他们已经完成了操作。
7. primary 回复客户端。在任何副本上遇到的任何错误都会报告给客户端。在有错误的情况中，write 可能已经在 primary 和部分 secondary 中成功完成了，（如果是 primary 这里失败了，那么其就不会指定序列号，也不会向 secondary 传递命令。<font color=red>（这里存在问题，因为按步骤4所说，是先指定序列号的，有没有可能是指定完序列号，应用修改时失败？）</font>）此时客户端会认为请求已经失败，已经修改完的区域就会处于 *inconsistent* 的状态。我们的客户端代码通过重试失败的修改来处理这种错误，即将在步骤 3 到 7 进行几次尝试，如果仍然没有成功，就会退回到 write 的开始时进行重试。<font color=red>（这里有个疑问是，已经完成操作或部分完成操作的副本，接收到重试的数据后，如何处理？）</font>



如果应用程序的 write 很大或者跨过了一个 chunk 的边界，GFS 客户端代码就会把其拆成多个 write 操作。这些新的 write 操作也都遵循上述控制流（Figure 2），但可能会与来自其他客户端的并发操作交错并被覆盖。因此，共享文件区域最终可能包含来自不同客户端的片段，尽管副本将是相同的，因为单个操作在所有的副本上以相同的顺序成功完成。这就会出现我们在 2.7 中提到过的 *consistent* 但 *undefined* 的状态。



### 3.2. 数据流

我们解耦了控制流和数据流以高效的使用网络。当控制流从客户端到 primary 再到所有的 secondary 时，数据是以流水线的方式沿着精心挑选的 chunkservers 链路线性 push 的。我们的目标是充分利用每个机器的网络带宽，避免网络瓶颈和高延迟链路，同时最小化 push 全部数据的延迟。

为了充分利用每台机器的带宽，数据是线性地沿着一个 chunkserver 链路 push 的，而不是分布式地 push （例如，树）。因此，每台机器全部的向外的带宽都被用来尽可能快的传输数据，而不是像分布式那样把数据在多个接受者之间分配。

为了尽可能的避免网络瓶颈和高延迟链路（例如，交换机见链路通常两者都有），每个机器向网络拓扑中“最近的”还没有收到数据的机器传递数据。假设客户端正在 push 数据，要将数据 push 到 chunkserver S1 到 S4。客户端先把数据发送到最近的 chunkserver，这里记为 S1。S1 将数据传递给 S2 到 S4 中离它最近的，这里记为 S2。类似的，S2 再把数据传递给 S3 或 S4，选择离它更近的，等等以此类推。我们的网络拓扑足够简单，因为“距离”可以通过 IP 地址来准确的估算。

最终，我们通过 TCP 连接流水线式的传输数据最小化了延迟。一旦一个 chunkserver 收到了一些数据，它就会立即向其他 chunkserver 传递传递这些数据。对我们来说，流水线式传输数据是非常有用的，因为我们使用的是全双工链路的交换网络。立即发送数据不能不会减小接受速率。不考虑网络拥堵的话，把 $B$ 字节数据发送到 $R$ 个副本的理想的时间消耗是 $B/T+RL$，$T$ 是网络吞吐量，$L$ 是在两个机器上传输字节的延迟。我们的网络链路通常是 100 Mbps ($T$)，$L$ 远远小于 1ms。因此，理想情况下，把 1 MB 散布出去需要大约 80ms。





### 3.3. 原子记录追加(append)

GFS 提供了一个原子的 append 操作，称为 *record append*。在传统的 write 操作中，客户端指定写入数据的偏移。对同一个区域的并发写不可以串行化：这个区域最终可能包含来自多个客户端的数据片段。而在 record apppend 中，客户端仅指定数据，由 GFS 选择偏移并把偏移返回给客户端，GFS 将在这个偏移处原子地 append 数据至少一次（即一个连续的字节序列）。这与在 Unix 中写入一个以 `O_APPEND` 模式打开的文件类似，多个写者并发写入的时候不会有冲突条件。

Record append 在我们的一些分布式应用程序（会有在不同的机器上的客户端并发 append 同一个文件的场景的分布式应用程序）中被重度使用。在这种情况下，如果客户端采用传统的 write，那么就额外需要复杂且昂贵的同步，例如通过一个分布式锁管理器。在我们的工作负载中，像这样的文件通常用作多生产者/单消费者队列，或包含来自很多不同的客户端的合并后的结果。

Record append 是修改操作的一种，也适合我们 3.1 中说到的控制流，只是在 primary 中有一点额外的逻辑。客户端将数据 push 给文件的最后一个 chunk 的所有副本，然后给 primary 发送请求。primary 检查如果将这个 record 记录 append 到当前的 chunk 是否会导致 chunk 的大小超过最大值（64 MB）。如果会超过，primary 就会将当前 chunk 填充至最大大小，并告诉所有 secondary 也这么做，然后回复客户端，说明这个操作将在下一个 chunk 上重试。（Record append 单次 append 的数据大小被限制为至多为 chunk 最大大小的 1/4，以将最坏情况下的碎片保持在可接受的水平。）如果在当前 chunk 上 append 这个 record 不会使得这个 chunk 的大小超过最大值（这是通常的情况），那么 primary 就会把这个 record 记录 append 到它的副本，然后告诉 secondary 在精确的偏移处  write 这些数据。最后将成功的通知回复给客户端。

如果一个 record append 在某个副本上失败了，客户端会重试操作。结果就是，同一个 chunk 的副本可能包含不同的数据，同一个 record，有的 chunk 中有完整的，有的只有一部分。GFS 不保证所有副本都完全一样（每个字节都一样），只会保证数据作为一个原子单位被写入至少一次。这个属性很容易从简单的观察中得出，即操作报告成功必须是数据在一些 chunk 的所有副本的同样的偏移处写入完成。进一步说，在操作报告成功后，所有的副本至少与 record 末尾一样长，因此未来的 record 将会被指定一个更高的偏移或一个不同的 chunk，即便后面会有一个不同的副本成为 primary。就我们的一致性保证而言，成功的 record append 操作写入数据的区域是 *defined*（因此也是 *consistent*），而没有完全成功写入的区域是 *inconsistent* 的（因此也是 *undefined* 的）。 我们在第 2.7.2 中讨论过，我们的应用程序可以处理 *inconsistent* 的区域。





### 3.4. 快照(snapshot)

快照操作几乎瞬间就可以制作一个文件或一个目录树（即，源）的拷贝，同时最大可能的减少中断正在进行中的修改。我们的用户使用快照来快速创建一个大数据集（经常是副本的副本<font color=red>（这段没读懂）</font>，递归）的分支副本，或者在试验修改前创建一个当前状态的检查点，稍后的提交或者回滚可以轻松些。

类似 AFS，我们使用**写时复制(copy-on-write)**技术来实现快照。当 master 收到一个快照请求时，先撤销要做快照的文件中的 chunks 的所有未到期的租约。这保证了任何后来的对于这些 chunks 的 write 操作需要和 master 互动以寻找一个租约持有者。这会让 master 有机会先去创建这个 chunk 的一个新的副本。

在租约被撤回或者到期以后，master 将操作记录在磁盘上。master 随后通过复制源文件或目录树的元数据将此日志记录应用于其内存状态。最新创建的快照文件指向与源文件相同的 chunk。<font color=red>（这段没读懂）</font>

在快照操作后，一个客户端第一次想要 write 一个 chunk `C`，要给 master 发送一个请求，为了得到当前的租约持有者。master 注意到 chunk `C` 的引用数大于 1<font color=red>（这里我的理解是，原文件对这个 chunk 有一个引用，创建的快照也对这个 chunk 有一个引用。不知道对不对。）</font>，于是推迟回复客户端请求并选择一个新的 chunk 句柄 `C'`。然后 master 让每个有 `C` 当前副本的 chunkserver 创建一个名为 `C'` 的新 chunk。通过在和原来相同的 chunkserver 上创建这个新的 chunk，我们可以确保数据可以在本地复制，而不经过网络（我们的磁盘速度大约是我们 100 Mb 以太网链路的三倍快）。从这一点看，对于任何 chunk 来说，请求操作一模一样：master 授予一个新的租约给新的 chunk `C'`，回复客户端，客户端可以正常 write 这个 chunk，而不知道这个 chunk 其实是刚刚从已存在的 chunk 新创建的。





## 4. master 操作

所有的命名空间操作都由 master 执行。

此外，master 对 chunk 副本的管理贯穿整个 GFS 系统：

1. master 决定在哪放置 chunks 副本；
2. 创建新的 chunks 和之后的副本；
3. 协调各种各样的系统范围内的活动以保持 chunks 完全拷贝；
4. 在所有的 chunkservers 上做复杂均衡；
5. 回收未使用的存储空间。

下面我们深入讨论下上述的几点。



### 4.1. 命名空间管理和锁

很多 master 的操作会花很长的时间。例如，一个快照操作必须撤回快照覆盖的所有 chunks 在 chunkserver 上的租约。在执行快照操作期间，我们不想推迟其他 master 的操作。因此，我们允许同时进行多个操作，并在命名空间的区域上使用锁来确保正确的操作执行顺序。

与很多传统的文件系统不同，GFS 没有按目录列出该目录中所有文件的数据结构，也不支持等价一个相同文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS 在逻辑上将其命名空间表示为将完整路径名映射到元数据的查找表。通过前缀压缩，这个表可以在内存中高效地表示。命名空间树中的每个结点（即绝对文件名或绝对路径名）都有一个与之相关联的读写锁。

每个 master 操作在执行前都要获取多个锁。一般来说，如果操作涉及 `/d1/d2/.../dn/leaf`，则该操作要请求目录名 `/d1`, `/d1/d2`, ..., `/d1/d2/.../dn` 上的读锁，以及完整路径名 `/d1/d2/.../dn/leaf` 的读锁或者写锁。注意 `leaf` 可能是一个文件，也可能是一个目录，具体取决于操作。

现在我们通过一个例子来讲解锁机制是如何工作的。在这个例子中，`/home/user` 正在被创建快照，快照将存到 `/save/user`，我们来说在这个过程中，锁机制是如何阻止 `/home/user/foo` 被创建的。快照操作要获取 `/home` 和 `/save` 上的读锁，以及 `/home/user` 和 `/save/user` 上的写锁<font color=red>（这里没明白为什么需要 /home/user 上的写锁）</font>。文件创建操作要获取 `/home` 和 `/home/user` 上的读锁，以及一个 `/home/user/foo` 上的写锁。这两个操作将会被按正确的顺序执行，因为他们尝试获取 `/home/user` 上冲突的锁（快照操作要的写锁与文件创建操作要的读锁）。文件创建操作不需要父目录（这里即 `/home` 和 `/home/user`）上的写锁，因为没有“目录”或类似 *inode* 那样的数据结构需要在修改操作中受保护，而命名空间上的读锁可以有效地确保父目录不会被删除（这里即为 `/home` 和 `/home/user` 上的读锁可以保护他们不被删除）。

上述锁策略的一个好处是，允许在同一个目录内的并发修改。举个例子，在同一个目录中可以并发的创建多个文件：每个创建文件操作获取一个目录名字上的读锁和一个文件名字上的写锁。目录名字上的读锁足以阻止目录被删除、重命名或被创建快照。文件名字上的写锁会连续两次尝试创建同名的文件<font color=red>（这句话黑人问号脸！）</font>。

由于命名空间可能会包含很多结点，所以读写锁对象是延迟分配的，并且一旦不用了就会被删除。另外，多个锁要以一个一致的总体顺序被获取，以防死锁：这些锁会先被按照命名空间树的级别排序，同级别之间则按照字典序。



### 4.2. 副本放置

一个 GFS 集群高度分布在多个级别上。GFS 集群往往在很多个机器机架上含有数百个 chunkservers 。这些 chunkservers 可能轮流被来自相同或不同机架上的数百个客户端连接。在不同机架上的两个机器间通信可能经过一个或多个网络交换机。此外，出入一个机架的带宽可能小于这个机架中所有机器的总带宽。多级分布提出了一个特别的挑战，即在保证可伸缩性、可靠性和可用性的前提下，分发数据。

chunk 副本的放置策略服务于两个目的：(1) 最大化数据的可靠性和可用性，(2) 最大化网络带宽利用率。为了实现这两个目的，仅仅跨机器传播副本是不够的，因为这只是能抵御磁盘或机器故障，以及能充分利用每个机器的网络带宽。我们必须也跨机架传播 chunk 副本，这可以确保即便整个机架都损坏了或者离线了（例如，由于共享资源故障，如网络开关或电源电路）。这也意味着，关于一个 chunk 的流量，尤其是读，可以充分利用多个机架的总带宽。另一方面，写流量必须流经多个机架，这是一个我们乐意看到的权衡。



### 4.3. 创建(Creation)、重新复制(Re-replication)、重新平衡(Rebalancing)

chunk 副本将在下面三种情况下创建：chunk 创建、重新赋值、重新平衡。

master 在**创建(*create*)**一个 chunk 时会选择一个位置来放置初始的空的副本<font color=red>（这里没理解初始的空的是啥意思）</font>。这考虑到了几个因素，(1) 我们想把新的副本放在磁盘空间利用率低于平均值的 chunkservers 上。随着时间推移，这个方法会使得各个 chunkservers 上的磁盘利用率相等。(2) 我们想限制在每个 chunkserver 上“最近”创建的数量。尽管创建操作本身开销很低，但创建操作会可靠的预测即将到来的大量的写流量，因为 chunk 是在写操作有要求时创建（这里就是说，在一个 chunkserver 上创建一个新的 chunk，创建操作本身开销不大，但是接下来往往会有写操作，如果“最近”创建的 chunk 太多，那么意味着后面会有太多的写流量，这会加重这个 chunkserer 的压力）。在我们的 append 一次读多次(append-once-read-many) 的工作负载中，当 chunk 被完全写入完成以后，通常会变成实际上的只读。(3) 像上面讨论的那样，我们想在跨机架传播一个 chunk 的副本。

当副本的有效数量低于用户指定的值时，master **重新复制(*re-replicates*)**一个 chunk。可能导致重新复制操作发生的原因多种多样：(1) 一个 chunkserver 变得不可用，则会给 master 报告它上面的副本可能损坏；(2) chunkserver 上的磁盘之一由于错误变得不可用；(3) 或者用户指定的副本数量增加了。

基于下面几个因素需要被重新复制的 chunk 会被优先处理：(1) 当前副本数量与目标副本数量相差太多。例如，相比丢失了一个副本的 chunk，我们会更优先处理丢失了两个副本的 chunk 的重新复制操作。(2) 我们倾向先处理存在的文件的 chunk 的重新复制操作，而不是最近删除的文件（详见 4.4）。(3) 最后，为了最小化故障对正在运行的应用程序带来的影响，我们提高任何使得客户端进程阻塞的 chunk 的优先级。

master 选择优先级最高的 chunk，通过指示一些 chunkservers 直接从一个现存的有效副本拷贝 chunk 数据来“克隆”这个 chunk。新副本的放置策略的目标和新建 chunk 的放置类似：均衡磁盘空间利用率，限制任一单个 chunkserver 上的活跃的克隆操作数，以及跨机架传播副本。为了防止克隆流量大于客户端流量太多，master 同时在整个集群上和每个 chunkserver 上限制活跃克隆操作的数量。此外，每个 chunkserver 通过减少其向源 chunkserver 的读请求来限制其花在每个克隆操作上的带宽总量。

最后，master 定期**重新平衡(*rebalances*)** 副本：master 检查当前副本的分布，然后移动一些副本，为了更好的利用磁盘空间，以及更好的负载均衡。通过这个过程，master 也可以逐渐填满一个新的 chunkserver，而不是立即用新的 chunks 和随之而来的大量写流量将其淹没。新副本的放置标准也和上面讨论过的类似。此外，master 也必须选择要移除哪个现存的副本。一般来说，master 倾向于移除那些空闲空间低于平均值的 chunkservers 上的 chunk，以平衡各个 chunkserver 磁盘空间的使用。



### 4.4. 垃圾回收



在一个文件被删除后，GFS 不会立即回收其有效的物理存储空间。master 只会在文件级别和块级别的垃圾回收期间，延迟回收物理存储。我们发现这个方法使得 GFS 系统更简单，更可靠。

#### 4.4.1. 机制

当一个文件被应用程序删除，master 和对其他的修改一样，立即记录删除日志。然而，这个文件只是被重命名为一个包含了删除时间戳的隐藏的名字，而不是立即回收了其资源。在 master 对文件系统命名空间的的定期扫描过程中，master 移除任何这样的，已经存在超过 3 天（内部可配置的）的隐藏文件。在这之前，被删除的文件仍然可以通过新的、特殊的名字（即被重命名后的带有删除时间戳的名字）被读取，也可以通过把名字改回正常名字取消删除。当隐藏文件被从命名空间中删除时，其内存中的元数据也会被删除。这有效地切断了它与所有 chunk 的链接。

类似的，在 master 对 chunk 命名空间的定期扫描中，master 识别孤儿 chunks（即不能从任何文件到达这个 chunk），并删除这些孤儿 chunks 的元数据。在与 master 的定期交换的心跳(*HeartBeat*)消息中，每个 chunkserver 报告其持有的 chunks 的一个子集，master 向 chunkserver 回复已经不在 master 存储的元数据中的所有 chunks 的身份信息，然后 chunkserver 就可以自由删除这些 chunks 的副本了。





#### 4.4.2. 讨论

尽管分布式垃圾回收是一个难题，需要在编程语言的上下文中解决复杂的问题，但对于我们的 GFS 来说相当简单。我们可以轻松识别 chunks 的所有引用：master 维护着专门的文件到 chunk 的映射。我们也可以轻松识别所有的 chunk 副本：所有的副本都在每个 chunkserver 下一个指定的目录中。另外，任何 master 不知道的副本都被视为“垃圾(garbage)”。

这种存储回收利用的垃圾回收方法，相比即时删除有几个优势。首先，在组件故障很常见的大规模的分布式系统中更简单更可靠。chunk 的创建可能在一些 chunkservers 上成功了，而在另一些 chunkserves 上失败了，留下了一些 master 不知道存在的副本<font color=red>（这里暂时的理解可能不准确。我记得是有一个失败了，就会重做所有的操作，那么成功创建了 chunk 的 chunkserver，其上的 chunk 就不被 master 认可，即 master 不知道的存在的副本）</font>。副本删除信息可能丢失，master 必须记得在失败时重新发送这些信息，包括 master 自己的和 chunkservers 的<font color=red>（这句没懂）</font>。垃圾回收提供了一个统一的、可靠的方法来清理任何未知有用的副本。第二，这种垃圾回收方法会将存储回收操作合并到 master 常规的后台活动中，就像定期扫描命名空间和与 chunkservers 定期握手一样。因此，存储回收操作是分批完成的，其开销分摊到了各个 master 常规的后台活动中。此外，存储回收操作只在 master 相对空闲的时候执行，这样 master 可以更迅速地响应需要及时关注的客户请求。第三，存储空间的延迟回收提供了一个防止意外的、不可逆的删除的安全网（这里我的理解是，意外删除的文件，在其被真正回收之前，是可以撤销删除的）。









### 4.5. 过期副本检测



