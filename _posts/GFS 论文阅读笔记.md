---
title: GFS 论文阅读笔记
date: 2021-12-28 23:31:39
updated: 2022-03-01 01:16:39
categories: [论文阅读笔记]
tags: [GFS,论文,分布式]
mathjax: true
toc: true
---



传送门: [GFS 论文原文](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/035fc972c796d33122033a0614bc94cff1527999.pdf)



## 1. 什么是 GFS



GFS，全称 Google File System，谷歌文件系统。

这篇论文是 2003 年发表的，在这之前，GFS 已经大规模应用在了 Google 内部。

GFS 是 Google 提出的一个文件系统，其是分布式的，主要用于处理越来越庞大的数据。因为当数据量大到一定程度时，传统的数据存储与处理方式就显得很笨重了，不适用了（比如你很难很快地读取数百 TB 的数据）。



<!--more-->





## 2. 设计概述



opportunity n. 机会，机遇

allude v. 略加提及

lay out 展示

promptly adv. 迅速地

contiguous adj. 连续的

arbitrary adj. 任意的

conscious adj. 意识到的

Performance-conscious 注重表现的，追求性能的

steadily adv. 逐渐地

forth adv. 向前

go back and forth 来回走动

seldom adv. 很少

well-defined adj. 定义明确的

semantics n. 语义学

essential adj. 基本的，必不可少的

simultaneously adv. 同时地

sustain v. 维持、保持

sustained adj. 持续的，持久的

latency n. 延迟

place a premiun on ... 重视 ...

stringent adj. 严格的，严厉的

hierarchically adv. 分层次地

invaluable adj. 无价的，极为宝贵的

flaky adj. 薄片的，不稳定的，脆弱的

lease 出租，租用

orphan n. 孤儿 v. 使成为孤儿 adj. 孤儿的

coherence n. 一致性

sophisticated adj. 复杂的

bottleneck n. 瓶颈，障碍物

translate A into B 把 A 翻译成 B

sidestep v. 回避

fragmentation n. 碎片

comfortably adj. 舒服的

stagger 交错

poll v.（计算机）轮询

no point 没有意义的

eternally adv. 总是；永恒地

even if 虽然；即使

respond v. 回应

thereby adv. 因此，从而

throughput n. 吞吐量

minute n. 分钟

or so 左右（eg: in a minute or so，在 1 分钟左右）

catastrophe n. 灾难

interference n. 干预

interspersed adj. 点缀的，散置的

mingle v. 混合

fragment n. 碎片，片段

distinguish v. 区别，辨别

purge v. 清除

premature adj. 过早的

irreversibly adv. 不可逆地

accommodate v. 提供住所、支持、容纳、适应

###### resilient adj. 有弹性的

perspective n. 视角，看法  adj. 透视的

keep ... from ... 阻止

lease v. 租约

grant v. 授予，同意，承认

piggybacked v. 驮运，附带

revoke v. 撤回，撤销

decouple v. 使分离

consecutive adj. 连续的，不间断的

straddles v. 跨过

interleave v. 交错

in a pipelined fashion 以流水线的方式

outbound adj. 向外去的

among prep. 在 ... 当中

congestion n. 拥堵

full-duplex 全双工

elapse v. 消逝 n. 时间的流逝

### 2.1. 假想（目标）

GFS 在设计的时候有一些假想，即预期要实现的目标。

1. 这个系统由很多廉价的、经常会故障的商用组件构建，所以在日常使用中，这个系统必须持续地监控自身，以检测、容忍组件故障，并迅速从组件故障中恢复。
2. 这个系统存储数量适中的大文件。Google 期望是几百万个文件，每个一般是 100MB 或者更大。数 GB 大小的文件在这个系统中也是很常见的，需要高效管理。而小文件肯定也要支持，但是不需要为了这些小文件专门优化。
3. 工作负载主要包括两类读：大文件流的读（流只能顺序读）和小文件的随机读。
    * 大文件流的读：单个读操作一般读几百 KB，更常见的是读 1MB 或者更多。来自同一个客户端连续的读操作经常是从一个文件连续的位置读。
    * 小文件的随机读：一般是在文件的任意位置读几 KB 大小。注重性能的应用程序通常对它们的小读取进行批处理和排序，以逐渐地浏览文件，而不是来回的读（文件指针来回移动）。
4. 这个系统也会有很多大的、连续的写操作，将数据追加到文件末尾。一般这种操作的大小和读差不多。一旦写入操作完成，这个文件很少会再次修改。小的随机写也支持，但是不太高效。
5. 这个系统必须高效地实现定义明确的语义，以支持多客户端并发写入（追加写入）同一个文件。GFS 中的文件通常用作生产者消费者队列或多路合并。系统中有数百个生产者，每个机器上运行一个，这些生产者并发地追加修改一个文件，因此以最小的同步开销来实现原子性是必不可少的。这些文件可能随后被读取，也可能有一个消费者在写的同时读。
6. 高的持续的带宽比低的延迟更重要。GFS 的大多数目标应用程序都重视以高速率批量处理数据，而很少有应用程序对单个读或写有严格的响应时间要求。





### 2.2. 接口

GFS 提供了一个常见的文件系统接口，尽管 GFS 没有实现像 POSIX 这样的标准 API。

GFS 中文件在目录中以层次结构组织，通过路径名区分。

GFS 支持常用操作以创建(create)、删除(delete)、打开(open)、关闭(close)、读(read)和写(write)文件。

此外，GFS 中还有 *snapshot* 和 *record append* 操作。Snapshot 以一个很低的开销创建一个文件的或者一个目录树的拷贝。Record append 允许多个客户端并发地追加写入同一个文件，且确保每个客户端的写入操作都是原子的。Record append 对实现多路合并结果、生产者消费者队列很有用，因为很多客户端可以同时追加写入，而不需要额外的锁。Google 发现在构建大型分布式应用时，这些类型的文件是非常有用的。

Snapshot 和 record append 会在后面进一步讨论。





### 2.3. 架构

一个 GFS 集群包含单个 *master* 和多个 *chunkservers*，允许多个 *client* 访问。如 Figure 1 所示。

![Figure 1: GFS Architecture](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Figure_1.png)

每个 master 或 chunkserver 一般都是一个商品 Linux 机器中运行着的一个用户级服务进程。在同一个机器上同时运行一个 chunkserver 和一个 client 是很容易，但前提是机器资源允许，并且你可以接受运行不稳定的应用程序代码导致的更低的可靠性。

GFS 系统中的文件会被划分为固定大小的 chunks。每个 chunk 使用一个不可变的、全局唯一的 64 位 chunk 句柄来标识，这个 chunk 句柄是在 chunk 创建时由 master 指定的。Chunkservers 在本地磁盘中以 Linux 文件的形式存储 chunks，并读取或写入由 chunk 句柄和字节范围指定的块数据。为了可靠性，每个 chunk 都在多个 chunkservers 上有复制。默认是 3 个复制，但用户可以为文件命名空间的不同部分指定不同的复制级别。

master 维护所有文件系统元数据，包括命名空间、访问控制信息、从文件到 chunk 的映射以及 chunks 当前的位置。master 也会控制系统范围内的活动，比如 chunk 租用管理，孤儿 chunks 的垃圾回收，以及在 chunkservers 之间迁移 chunks。master 会定期在 HeartBeat 消息中与每个 chunkservers 通信，以给 chunkservers 指令并收集其状态信息。

链接到每个应用程序的 GFS 客户端代码中实现了文件系统 API，这个 GFS 客户端代表应用程序与 master 和 chunkservers 通信以读写数据。客户端与 master 交互以进行元数据操作，但所有数据承载通信直接进入 chunkservers。GFS 没有提供 POSIX API，因此不需要连接到 Linux 的 vnode 层。

客户端和 chunkserver 都不缓存文件数据。客户端缓存文件数据几乎没什么好处，因为大多数应用程序通过巨大的文件进行流式传输，或者工作集太大而无法缓存。不缓存文件数据使得客户端代码和总体系统的代码得以简化，因为无需编写代码解决缓存一致性的问题（不过客户端是缓存元数据的）。Chunkservers 不需要缓存文件数据是因为 chunks 是作为本地文件存储的，所以 Linux buffer 缓存已经把频繁访问的数据放在内存中了。



### 2.4. 单个 Master

GFS 中只有一个 master，这大大简化了其设计，并且使得 master 能够根据全局知识做出复杂的 chunk 放置和复制决策。不过必须最小化在读写中 master 的调用次数，防止 master 成为 GFS 系统的性能瓶颈。客户端永远都不会通过 master 读写文件数据，而是向 master 询问该联系哪些 chunkservers，当客户端会在有限的时间内缓存此信息，且直接和 chunkservers 互动，以进行一系列的操作。

---

现在我们通过一个简单的读操作来解释 GFS 的工作流程（就如 Figure 1 中的那样）。

首先，要使用固定的 chunk 大小，客户端把应用程序指定的文件名和字节偏移翻译成这个文件中的一个 chunk 索引。然后客户端向 master 发送一个包含文件名和 chunk 索引的请求，master 给客户端回复相应的 chunk 句柄和 chunk 副本的位置。客户端以文件名和 chunk 索引作为 key 缓存这些信息。

客户端随后给副本之一发送一个请求（大部分情况是最近的一个副本），这个请求中指定了 chunk 句柄和一个 chunk 中的字节范围。同一个 chunk 的读就不再需要 client-master 互动了，直到客户端缓存的信息到期（前文说过在有限的时间内缓存这些信息，也就是说这些信息是有时效性的）或这个文件被重新打开。事实上，客户端往往在一个请求中询问多个 chunks，master 也可以在回复的信息中心包含这些请求的 chunks 信息，这些额外的信息几乎不需要什么额外的开销，就可以避免未来几次的 client-master 交互。





### 2.5. Chunk 大小



Chunk 的大小是关键的设计参数之一。GFS 中将 chunk 的大小设定为 64MB，远远大于一般文件系统的块大小。每个 chunk 副本都以一个普通的 Linux 文件存储在一个 chunkserver 上，只要需要的时候才会扩展。延迟空间分配避免了由于内部碎片造成的空间浪费，这可能是对如此大 chunk 大小的最大反对。

将 chunk 设置为 64MB 这么大，可以提供一个重要的优势。**首先**，减少了客户端与 master 的交互次数，因为在同一个 chunk 上的读和写只需要在最初的请求中向 master 询问一次 chunk 的位置信息。减少客户端与 master 交互次数对于我们的工作负载而言格外重要，因为应用程序往往是连续读写大文件的。即便是对于小的随机读，客户端也可以轻松缓存一个数 TB 工作集的所有 chunk 的位置信息。**第二**，由于一个 chunk 比较大，使得一个客户端更可能在一个给定的 chunk 上执行很多操作，这样就可以在很长的一段时间内，通过保持一个持续的客户端与 chunkserver 之间的 TCP 连接来减少网络开销。**第三**，减少了 master 上存储的元数据大小。这允许我们把元数据放在内存中，把元数据放在内存中又反过来带给我们一些其他的优势，这些优势我们在 2.6.1 中讨论。

另一方面，一个很大的 chunk 大小，即便有延迟空间分配策略，也还是有缺点的。一个小文件可能包含很少数量的 chunks，甚至可能只有一个。这样如果有很多客户端都要访问这同一个文件，那么存储这些 chunks 的 chunkservers 就会成为热点。不过在实践中，热点问题不是主要问题，因为我们的应用程序大多是顺序读多 chunk 的大文件。

然而，当 GFS 首次被批处理队列系统使用时，热点确实出现了：一个可执行文件作为单个 chunk 文件写入 GFS，然后同时在数百台机器上启动。存储此可执行文件的少数 chunkservers 被数百个同时请求过载。Google 通过以更高的复制因子存储此类可执行文件以及使批处理队列系统错开应用程序启动时间来解决此问题。一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据。





### 2.6. 元数据



master 中主要存储三种类型的元数据：

1. 文件和 chunk 的命名空间；
2. 从文件到 chunks 的映射；
3. 每个 chunk 的副本的位置。

所有的元数据都存储在 master 的内存里。前两种类型也会通过在操作日志(operation log)上记录修改来持久化，操作日志存储在 master 的本地磁盘上，并且会在远程机器上复制。使用日志使得我们可以容易地、可靠地更新 master 状态信息，而不用承受 master 崩溃导致的不一致性的风险。master 不会持久的存储 chunk 位置信息，而是会在 master 启动时或一个 chunkserver 加入集群时向 chunkserver 询问其 chunks 信息。



#### 2.6.1. 内存中的数据结构

因为元数据存储在内存中，所以 master 的操作是非常快速的。进一步地说，master 定期在后台扫描其整个状态信息是非常简单且高效的。定期扫描是用来实现 chunk 垃圾回收、chunkserver 故障时的重新复制，以及为了负载均衡和跨 chunkserver 使用磁盘空间进行的 chunk 迁移的。4.3 和 4.4 小节会进一步讨论这些内容。

仅在内存访问这些，有一个潜在的问题是，chunks 的数量和整个 GFS 系统的容量受 master 拥有多少内存限制。在实践中这不是一个很严重的限制。对于每个 64MB 的 chunk，master 维护小于 64 字节的元数据。大部分 chunks 是满的，因为大部分文件包含很多个 chunks，只有最后一个可能是不满的。类似地，对于每个文件，master 存储的文件命名空间数据通常少于 64 个字节，因为它使用前缀压缩紧凑地存储文件名。

如果确有必要支持更大的文件系统，只需要给 master 增加额外的内存，这个开销相对于我们在内存中存储元数据获得的简单性、可靠性、性能与灵活性而言，是很小的。



#### 2.6.2. Chunk 的位置



master 不会持有一个持久的关于哪些 chunkservers 有一个给定 chunk 的副本的记录，而是在 master 启动时简单地轮询 chunkservers 来获取这些信息。启动后 master 可以保持最新，因为 master 控制着所有 chunk 的放置，以及通过常规心跳(HearBeat)消息监控着 chunkserver 的状态。

Google 起初尝试在 master 中持久存储 chunk 的位置信息，但是后来决定在 master 启动时（以及启动后定期）从 chunkmasters 请求数据，这简单的多。并且这样做也排除了在有 chunkservsers 加入或离开集群、修改名字，故障、重启时等等保持 master 和 chunkservers 同步的问题。在一个有着数百个服务器的集群上，这些情况常常发生。

另一个理解这样设计决策的思路是这样想，一个 chunkserver 有决定存储哪些 chunks 在其本地磁盘上的最终话语权。在 master 上尝试维护一个这种信息的一致性视图是没有意义的，因为一个 chunkserver 上的错误可能导致 chunk 自发消失（比如磁盘损坏或不可用），或操作员可能修改 chunkserver 的名字。





#### 2.6.3. 操作日志(Operation Log)



操作日志包含至关重要的元数据修改历史记录。

操作日志是 GFS 的核心。操作日志不仅仅是元数据唯一的持久化记录，也是一个逻辑时间线（充当定义并发操作的次序）。文件和 chunks 还有它们的版本(versions，详见 4.5. 小节)，全部由他们被创建时的逻辑时间唯一且永久标识。



由于操作日志是非常重要的，我们必须将其可靠存储，并在在元数据修改持久化之前不让修改对客户端可见。否则，我们会在事实上丢失整个文件系统或最近的客户端操作（即便 chunks 本身还在）（这里原文翻译过来就是这样的。我的理解是客户端对 chunks 的操作依赖其缓存的元数据，如果元数据的改动在持久化前就对客户端可见的话，客户端就会依赖改动后的元数据对 chunks 操作，而此时这些元数据的改动还没有持久化，客户端的操作可能无法执行，导致操作丢失）。因此 GFS 将操作日志在多个远程机器上复制，并且仅在相应的日志记录已经被 flush 到本地和远程磁盘上后才会响应一个客户端的操作。Master 在 flush 前一起批处理几个 log 记录，从而减少 flush 和复制对整个系统吞吐量的影响。

Master 通过重放操作日志来恢复其文件系统状态信息。为了使 master 启动时间最短，就要保持日志小。每当日志超过一个特定的大小时，master 就会生成一个包含其此时状态的检查点(check point)，这样 master 恢复的时候，只需要从本地磁盘加载最近的检查点，然后重放在这个检查点之后的有限数量的日志记录。检查点采用类似 B 树的紧凑形式，可以直接映射到内存中，用于命名空间查找，无需额外解析。 这进一步加快了恢复速度并提高了可用性

因为构建一个检查点需要一些时间，所以 master 的内部状态通过这样的一种方式构造：创建一个新的检查点时不推迟即将到来的修改，master  会切换到一个新的日志文件，并且在一个单独的线程中创建新的检查点。换句话说，新的检查点中包含了切换前的所有修改，切换后的修改会被记录到 master 切换过去的新的日志文件中。对于一个有几百万文件的集群来说，可以在一分钟左右创建完一个新的检查点。当检查点创建完成时，检查点会被写入本地和远程的磁盘。

恢复操作只需要最近的检查点和日志文件序列。更旧的检查点和日志文件就可以随便删了，尽管一般来说会保留一些以抵御灾难。在创建检查点时发生的故障不会影响正确性，因为恢复代码会检查并跳过不完整的检查点。





### 2.7. 一致性模式

GFS 有一个宽松的一致性模型，很好地支持我们的高度分布式应用程序，但是实现起来依然简单且高效。

我们现在讨论 GFS 如何保证一致性，以及这对应用程序来说有何意义。我们也会强调 GFS 如何维护这些保证，但是更详细的内容将在本文的其他部分来说。

#### 2.7.1. GFS 如何保证一致性

文件命名空间的修改（例如，文件创建）是原子的，且只能由 master 来操作：命名空间锁确保原子性和正确性（详见 4.1）；master 的操作日志定义了一个这些操作的全局的总的次序（详见 2.6.3）。

在数据修改后，文件域的状态依赖于修改的类型，修改成功还是失败，以及这些是否是并发的修改。Table 1 总结了在数据修改后的文件域的状态。

![Table 1: File Region State After Mutation](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Table_1.png)

* 对于一个文件域，如果所有的客户端总是看到相同的数据（不论看的是哪个副本），那这个文件域是一致的 *consistent*。

* 对于一个文件域，在文件数据修改后，如果这个修改是一致的，并且客户端将看到这个修改写入的全部内容，那么这个文件域就是 *defined*。

（个人理解：对于一个文件域，只要所有客户端看到的数据都是一样的，那这个域就是 consistent 的。在 consistent 的前提下，如果所有修改都已经被写入，就是 defined 的。consistent 是 defined 的子集。即 defined 的一定是 consistent 的，但 consistent 的不一定是 defined 的（上表中的 Recored Append 在后面单独说）。）

当一个修改成功，且没有受到并发写者的干预（即串行的修改），那么受影响的域是 *defined* 的（且含义一致）：即所有的客户端将总是能看到这个修改写入了什么。

并发的成功的修改使得受影响的域是 *undefined* 但 *consistent*：即所有的客户端看到的数据是一样的，但这并不意味着每个修改都已经被写入。一般来说，写入的内容由多个修改的混合片段组成。

一个失败的修改会使得文件域 inconsistent（因此也是 undefined）：不同的客户端在不同的时间可能看到不同的数据。我们在下面描述我们的应用程序如何辨别 defined 的域和 undefined 的域。另外，应用程序不需要进一步区分不同种类的 undefined 的区域。

数据修改可能是 *write* 或 *record appends*。

* *write* 使数据被写入在一个由应用程序指定的文件偏移处。
* *record append* 使数据（即 *record*）被**原子地**的追加至少一次（即便是并发修改），但数据写入的文件偏移由 GFS 选择（详见 3.3）。

作为对比，一个普通的 append 仅仅是一个在客户端认为是当前文件末尾的偏移处的 write。

标志着包含写入 record 的 *defined* 的区域的开始的偏移会被返回给客户端。此外，GFS 可能会在写入的内容之间插入填充或 record 的复制。我们认为 GFS 插入内容占据的域是 *inconsistent* 的，且占用的空间比起用户数据的总量而言微不足道。

在连续的成功的修改后，GFS 会保证被修改的文件域是 *defined* 的，并且包含最后一次修改写入的数据。GFS 实现这一点，通过 (a) 以相同的顺序应用修改到 chunk 以及其所有的拷贝上（详见 3.1），(b) 使用 chunk 版本号检测某个拷贝是否过期（即在其对应的 chunkserver 挂掉时，错过了修改。详见 4.5）。过期的 chunk 拷贝永远都不会被再应用修改，其位置也不会再由 master 提供给客户端，这些过期的 chunk 将尽快被垃圾回收。

由于客户端缓存了 chunk 的位置信息，所以在其缓存的位置信息更新之前，客户端可能会从一个旧的副本中读取数据。只有当缓存条目超时，或文件被重新打开时，这个问题才能解决，因为条目超时或重新打开文件会清除客户端缓存中的所有跟这个文件有关的 chunk 信息。此外，由于我们的文件大多数都是仅 append 的，一个旧的副本通常返回一个最新的 chunk 结束位置之前的位置，而不是过期的数据（也就是说，数据还是有效的数据，只是返回的偏移位置不对）。当一个读者重试并联系 master 时，读者会立即获得现在的 chunk 的位置。

即便在修改成功后的较长时间后，组件故障仍然可以导致数据被损坏、催毁。GFS 通过 master 与所有 chunkservers 定期握手的方式来找到故障的 chunkservers，通过校验和（详见 5.2）来检测数据是否损坏。一旦发现问题，GFS 会尽快通过相应数据的其他有效副本来恢复数据（详见 4.3）。仅当一个 chunk 的所有副本都丢失了，这个 chunk 的丢失才是不可逆地，即便在这种情况下，chunk 也是无法访问，而不是损坏：应用程序会收到一个明确的错误，而不是损坏的数据。





#### 2.7.2. 对应用程序的影响

GFS 应用程序可以用一些其他目的已经需要的简单技术来适应宽松的一致性模型：依赖 append 而不是覆写、检查点，和写入自验证、自识别的记录。

实际上，我们应用程序所有的文件修改，都是通过 append 而不是覆写。

一个典型的用法是，一个写者从头到尾生产一个文件，在数据全部写入完成后，在原子地将文件重命名一个持久化的名字。或者是定期生成检查点，即每成功写入多少数据，就生成一个检查点。检查点也可能包含应用程序级检查和。

读者只验证和处理直到最新的检查点的文件域，即已知的 *defined* 状态的文件域。无论一致性和并发问题怎样，这种方法都很好地为我们服务。

对应用程序故障而言，append 比随机写要高效、弹性得多。

检查点允许写者递增的重新开始（即可以从更新的检查点处接着写），阻止读者处理已经成功写入，但还未对应用程序可见（即在应用程序认为还不完整）的数据。

另一个典型的用法，很多写者并发 append 一个文件，以获取合并结果或作为一个生产者-消费者队列。*Record append* 的 **append 至少一次**语义保留了每个写者的输出。

读者通过下面的方法来处理偶然的填充或者 record 的复制。每个 record 由写者准备好，包含了诸如校验和这种额外信息，这样 record 的有效性就可以验证。读者使用校验和，可以区分填充和 recored 片段。如果应用程序不能容忍偶尔的重复（例如，如果重复的记录会触发非幂等操作），它可以使用记录中的唯一标识符将它们过滤掉，这通常是命名相应应用程序实体（例如 Web 文档）所必需的。这些 record I/O 的功能（除了重复记录的移除）在我们应用程序共享的库代码中，并且也适用其他 Google 的文件接口实现。这样，相同的 record 序列加上极少的重复，总是会被传送给 record 读者。



## 3. 系统交互

Google 设计 GFS 系统交互要最小化在所有的操作中对 master 的涉及（因为 master 只有一个，必须减轻 master 的压力）。在这个背景下，我们现在来说客户端、master 和 chunkserver 如何互动以实现数据修改、原子记录追加(append)，以及快照(snapshot)。



### 3.1. 租约和修改顺序



像 write 或 append 的修改操作是会改变 chunk 的内容或者元数据的。每次修改都会应用在 chunk 的所有副本上。我们使用租约来维护一个在副本之间一致的修改顺序。master 授予一个 chunk 租约给副本之一，我们称这个副本为 *primary*。Primary 为所有修改挑选一个顺序给 chunk。当应用修改时，所有的副本都遵循这个顺序。<font color=red>(这里不太确定，回头再来确认）</font>因此，全局修改顺序先由 master 选择的租约授予顺序定义，在租期内由 priamry 指定的序列号定义。

租约机制是设计用来最小化 master 的管理开销的。一个租约有一个初始的 60s 的超时时间。然而，只要 chunk 被修改，primary 就可以向 master 请求延时，并且通常会收到延时的许可，并且这不限制次数。这些扩展请求与授权是附带在 master 和所有 chunkservers 之间交换的常规的心跳(*HeartBeat*)信息中的。master 有时可能会尝试在一个租约到期前将其撤销（例如，master 想用禁用一个正在重命名的文件上的修改）。即便 master 与一个 primary 失去了联系，master 也可以在旧的租约到期后安全地向另一个副本授予租约。

在 Figure 2 中，我们通过列出 write 控制流描述了这个过程，并且用数字标记了步骤顺序。

![Figure 2: Write Control and Data Flow](https://gukaifeng.cn/posts/gfs-lun-wen-yue-du-bi-ji/GFS_Figure_2.png)

1. 客户端向 master 询问，哪个 chunkserver 持有要访问的 chunk 当前的租约，以及其他副本的位置。如果目前没有任何一个 chunkserver 持有要访问的 chunk 的租约，master 就会选择一个副本，授予一个租约（没有在图上显示出）。
2. master 向客户端回复 primary 的标识和其他副本（图中 *secondary* 标记，所有除了 primary 的副本都是 secondary）的位置。客户端缓存这个数据，用于将来的修改操作。只有当 primary 变得不可达，或副本不再持有租约时，客户端才需要再次联系 master。
3. 客户端把数据推送给所有的副本，客户端可以以任意的顺序推送。每个 chunkserver 将会在一个内部的 LRU buffer 缓存这些数据，直到这些数据被使用或老化。通过将数据流与控制流解耦，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不管哪个 chunkserver 是 primary 的。我们会在 3.2 进一步讨论这一点。
4. 一旦所有的副本都确认收到了数据，客户端就向 primary 发送一个 write 请求。这个请求标识了早前推送给所有副本的数据。primary 给其收到的所有修改指定连续的序列号，由于这些修改可能来自多个客户端，所有进行编号是有必要的。primary 按着序号的顺序将修改应用到自己的本地状态。
5. primary 把 write 请求传递给所有的 secondary 副本，每个 secondary 副本以由 primary 指定的同样的序列号顺序应用修改。
6. 向 primary 回复的所有 secondary，表明他们已经完成了操作。
7. primary 回复客户端。在任何副本上遇到的任何错误都会报告给客户端。在有错误的情况中，write 可能已经在 primary 和部分 secondary 中成功完成了，（如果是 primary 这里失败了，那么其就不会指定序列号，也不会向 secondary 传递命令。<font color=red>（这里存在问题，因为按步骤4所说，是先指定序列号的，有没有可能是指定完序列号，应用修改时失败？）</font>）此时客户端会认为请求已经失败，已经修改完的域就会处于 *inconsistent* 的状态。我们的客户端代码通过重试失败的修改来处理这种错误，即将在步骤 3 到 7 进行几次尝试，如果仍然没有成功，就会退回到 write 的开始时进行重试。



如果应用程序的 write 很大或者跨过了一个 chunk 的边界，GFS 客户端代码就会把其拆成多个 write 操作。这些新的 write 操作也都遵循上述控制流（Figure 2），但可能会与来自其他客户端的并发操作交错并被覆盖。因此，共享文件域最终可能包含来自不同客户端的片段，尽管副本将是相同的，因为单个操作在所有的副本上以相同的顺序成功完成。这就会出现我们在 2.7 中提到过的 *consistent* 但 *undefined* 的状态。



### 3.2. 数据流

我们解耦了控制流和数据流以高效的使用网络。当控制流从客户端到 primary 再到所有的 secondary 时，数据是以流水线的方式沿着精心挑选的 chunkservers 链路线性推送的。我们的目标是充分利用每个机器的网络带宽，避免网络瓶颈和高延迟链路，同时最小化推送全部数据的延迟。

为了充分利用每台机器的带宽，数据是线性地沿着一个 chunkserver 链路推送的，而不是分布式地推送（例如，树）。因此，每台机器全部的向外的带宽都被用来尽可能快的传输数据，而不是像分布式那样把数据在多个接受者之间分配。

为了尽可能的避免网络瓶颈和高延迟链路（例如，交换机见链路通常两者都有），每个机器向网络拓扑中“最近的”还没有收到数据的机器传递数据。假设客户端正在推送数据，要将数据推送到 chunkserver S1 到 S4。客户端先把数据发送到最近的 chunkserver，这里记为 S1。S1 将数据传递给 S2 到 S4 中离它最近的，这里记为 S2。类似的，S2 再把数据传递给 S3 或 S4，选择离它更近的，等等以此类推。我们的网络拓扑足够简单，因为“距离”可以通过 IP 地址来准确的估算。

最终，我们通过 TCP 连接流水线式的传输数据最小化了延迟。一旦一个 chunkserver 收到了一些数据，它就会立即向其他 chunkserver 传递传递这些数据。对我们来说，流水线式传输数据是非常有用的，因为我们使用的是全双工链路的交换网络。立即发送数据不能不会减小接受速率。不考虑网络拥堵的话，把 $B$ 字节数据发送到 $R$ 个副本的理想的时间消耗是 $B/T+RL$，$T$ 是网络吞吐量，$L$ 是在两个机器上传输字节的延迟。我们的网络链路通常是 100 Mbps ($T$),$L$ 远远小于 1ms。因此，理想情况下，把 1 MB 散布出去需要大约 80ms。





### 3.3. 原子记录追加(append)





### 3.4. 快照(snapshot)







## 4. master 操作

### 4.1. 命名空间管理和锁



### 4.2. 副本放置





### 4.3. 创建、再复制、再平衡



### 4.4. 垃圾回收

#### 4.4.1. 机制



#### 4.4.2. 讨论





### 4.5. 过期副本检测



